{"title":"Drawing inference from statistical models, and statistical power","markdown":{"yaml":{"output":"html_document","editor_options":{"chunk_output_type":"console"},"editor":{"markdown":{"wrap":"sentence","bibliography":"bibliography.bib"}}},"headingText":"Drawing inference from statistical models, and statistical power","containsRefs":false,"markdown":"\n\n\nThe study was set up as a statistical laboratory, were we performed simulations.\nThe purpose of this rapport is to interpret and explain the results we got.\n\n## Method\n\nWe simulated a population of possible values and then drew random samples, calculated statistics and interpreted them.\nThe population of values was regarded as the possible difference between two treatments in a cross-over study where participants performed both treatments.\nThe values in the population were calculated as Treatment - Control.\nWe simulated a population of one million numbers with a mean of 1.5 and a standard deviation of 3.\nWe then made two different set of studies, one set with a sample size of 8 (samp1) and one set with a sample size of 40 (samp2).\nAdditionally, we estimated the average value of the population.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation <- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 <- lm(y ~ 1, data = samp1)\nm2 <- lm(y ~ 1, data = samp2)\n\nsummary(m1)\nsummary(m2)\n```\n\nHere we drew two random samples corresponding sample sizes of 8 and 40 and saved this data in data frames with the dependent variable y.\nThen the model were fitted as a linear model and saved as a model object.\nObject m1 corresponds to a sample size of 8, while m2 corresponds to a sample size of 40.\nOur null hypothesis is that there is no difference between the two treatments.\n\n## Results\n\n### Explain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\n\nIn our model, the estimate represents the mean of the differences between the two treatments in the cross-over study.\nIn model m1, the estimate is 1.84.\nThis means that the average difference between the two treatments for the sample of 8 participants is 1.84.\nIn model m2, the estimate is 1.5642, meaning that the average difference between the two treatments for the sample of 40 participants is a little lower than for the sample of 8.\nFurthermore, the standard error (SE) provides an estimate of how much variability we expect in the sample mean if we were to repeatedly draw samples of the same size from the population.\nIt is calculated as the sample's standard deviation (SD) divided by the square root of the sample size.\nIn m1, the standard error (SE) is 1.251, which tells us how much the sample mean of 1.84 might vary if we were to repeat the study multiple times with a sample size of 8.\nIn m2, the standard error is 0.4774 indicating that the sample size of 40 participants gives us a more precise estimate of the population mean.\nThe t-value is a ratio that compares the difference between the sample mean (estimate), and the null hypothesis relative to the standard error (SE).\nIn m1, the t.value is 1.47, meaning the observed mean difference (1.84) is 1.47 standard errors away from the null hypothesis value of 0.\nThe t.value for m2 is almost three times bigger as it is 3.276 indicating that the observed mean difference (0.4774) is 3.276 standard errors away from the null hypothesis.\nLastly, the p-value tells us the probability of observing a t-value as extreme (or more extreme) than the one calculated, assuming the null hypothesis is true (i.e., no difference between treatments).\nIn m1, the p-value is 0.185, meaning that there is an 18.5% chance of observing a difference of 1.84 or more if the true difference between the treatments was zero.\nSince this p-value is above the conventional threshold of 0.05, we fail to reject the null hypothesis, suggesting that the observed difference is not statistically significant.\nIn m2, the p-value is 0.00221, meaning that there is an 0.221% chance of observing a difference of 1.5642 or more if the true difference between the treatments was zero.\nIn comparison to m1, this p-value is below the conventional threshold of 0.05.\nWe therefore reject the null hypothesis, suggesting that the observed difference is statistically significant and there is evidence that the means differ.\n\n### Discuss what contributes to the different results in the two studies (m1 and m2).\n\nThe two studies differ primarily in sample size, where m2 have 5 times more participants than m1.\nSince m1 have a small sample, the mean might fluctuate more due to random variation, whereas larger samples (m2) tend to provide a more stable and reliable estimate closer to the true population mean [@faber2014].\nFurthermore, the larger the sample size, the smaller is the standard error, which means a more precise estimate of the population mean [@faber2014].\nIn m2, the larger sample size leads to a smaller standard error (0.4774), which reduces the uncertainty around the estimate and increases the power of the test to detect differences.\nThe t-value is influenced by both the estimate and the standard error.\nEven if the estimates are somewhat similar, the smaller standard error in m2 results in a larger t-value, making it more likely to detect a significent effect.\nAdditionally, the p-value depends on the t-value.\nWith a larger sample size, as in m2, the t-value is typically larger, leading to a smaller p-value.This means that m2 is more likely to detect significant differences than m1, where the small sample size leads to a higher p-value and lower statistical power.\nIn conclusion, the larger sample size in m2 leads to a more precise estimate, a smaller standard error, a higher t-value, and ultimately a smaller p-value, increasing the likelihood of detecting a significant difference between the treatments.\n\n### Why do we use the shaded area in the lower and upper tail of the t-distribution.\n\nThe shaded area in the lower and upper tail of the t-distribution represents the probability of observing extreme values (both high and low) of the t-value under the null hypothesis.\nThis area helps us determine the p-value, which tells us how likely it is that our observed data could occur by random chance if the null hypothesis is true.\n\nThe total shaded area in both tails represents the combined probability of observing a t-value as extreme as the one calculated, assuming the null hypothesis is true.\nThe p-value for m1 where 0.185, meaning that 18.5% of the area is in the combined tails, representing the threshold for statistical significance.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults <- bind_rows(results_8, results_40)\n\n\n```\n\n### Calculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Calculate the standard deviation of the estimate variable for sample size 8 and 40\nsd_estimate_8 <- sd(results_8$estimate)\nsd_estimate_40 <- sd(results_40$estimate)\n\n\n# Calculate the average of the SE variable for sample size 8 and 40\navg_se_8 <- mean(results_8$se)\navg_se_40 <- mean(results_40$se)\n\n\n```\n\nBy calculating the standard deviation of the estimates across the 1000 studies we get a measure of how much the sample means fluctuate between different samples of the same size.\nFor the smaller sample size of 8, the standard deviation comes out to be `r sd_estimate_8`, while for the bigger sample size of 40 it comes out to be `r sd_estimate_40`.\nFurthermore, the average standard error represents the average uncertainty of the sample mean estimate in each study.\nIt reflects the variability in the sample means and depends on the sample size.\nThe average standard error for the smaller sample size is `r avg_se_8`, and `r avg_se_40` for the bigger sample size.\nWhy are these numbers very similar?\nThe standard deviation of the estimates and the average standard error are conceptually related, as the standard error estimates how much the sample mean might vary from the population mean.\nThey are both measures of variability, but while standard error (SE) is an estimate based on the sample, the standard deviation of the estimates shows the actual observed variation across multiple studies.\nTherefore, in light of these calculations, we can define standard error (SE) as the expected variability from sample to sample.\n\n### Create a histogram of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# A two facets histogram can be created with ggplot2\nresults %>%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n```\n\nFor sample size 8, the p-values are more spread out, with many values above the significance threshold of 0.05.\nThis indicates that with a smaller sample, there is lower statistical power, and many studies fail to detect a statistically significant difference.\nFor sample size 40, the p-values are more concentrated towards lower values, indicating that a larger sample size increases the likelihood of detecting significant effects.\nThis reflects increased statistical power with larger sample sizes, meaning the test is more likely to reject the null hypothesis when a true effect exists.\n\n### Calculate the number of studies from each sample size that declare a statistical significant effect.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Count the proportion of tests below a certain p-value for each \nresults %>%\n  filter(pval < 0.05) %>%\n  group_by(n) %>%\n  summarise(sig_results = n()/1000)\n\n```\n\nWhen calculating the number of studies that declare a statistical significant effect we find that fewer studies (0.227) are likely to reach statistical significance in the sample size of 8.\nThis is because of the low statistical power associated with small samples.\nIn the sample size of 40, more studies (0.865) will show a significant effect due to the higher power of larger samples, making it easier to detect true differences.\n\n### Using the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 8, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\n```\n\nTo calculate the power of a one-sample t-test, we use the effect size 𝑑 = 𝜇/𝜎, where 𝜇 = 1.5 (mean) and σ = 3 (standard deviation).\nThe effect size 𝑑 = 1.5/3 = 0.5.\nFor a sample size of 8 we find that the power will be relatively low, 0.232077, reflecting that with smaller samples, we have a lower chance of detecting a true effect when it exists [@faber2014].\nThis corresponds to fewer studies achieving significance in the simulation.\nFor a sample size of 40 we find that the power will be much higher, 0.8693981, indicating that with a larger sample size, we have a higher chance of detecting a true effect.\nThis explains why more studies with sample size 40 declare significance in the simulation.\n\nWe will now simulate a population without differences between treatment and control.\nThe code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\npopulation <- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null <- bind_rows(results_8, results_40)\n\n#create new histogram \n\nresults_null %>%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n  \n```\n\n### With a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Count number of false positives for sample size 8 (p-value < 0.05)\nfalse_positives_8 <- sum(results_8$pval < 0.05)\n\n# Count number of false positives for sample size 40 (p-value < 0.05)\nfalse_positives_40 <- sum(results_40$pval < 0.05)\n\n\n\n```\n\nTo determine how many studies would give a \"false positive\" result with a significance level of 5%, we calculate how many studies produce a p-value less than 0.05 in this simulation.\nSince the population mean is set to 0 any p-value below 0.05 in this scenario would represent a false positive.\nFor the sample size of 8 we got `r false_positives_8` while for the sample size of 40 we got `r false_positives_40`.\nWith a significance level of 5%, we expect roughly 5% of the 1000 studies to produce p-values below 0.05 due to random variation, even though the null hypothesis is true.\nFor 1000 studies, this means we expect around 50 false positive results for each sample size.\nThe actual number of false positives may vary slightly due to the randomness in the simulation but should be close to 50 for each sample size (8 and 40).\n\n## Conclusion\n\nIn this study, we explored the impact of sample size on statistical results by conducting simulations across different study sizes (8 and 40 participants).\nWe observed that larger sample sizes lead to more precise estimates, lower standard errors, and increased t-values, resulting in a higher likelihood of detecting statistically significant effects.\nOn the other hand, smaller sample sizes had higher variability in estimates and standard errors, contributing to lower statistical power and fewer significant results.\nOur findings also emphasize the importance of sample size in improving the reliability of statistical tests, as demonstrated by the power calculations.\nLarger samples provide greater statistical power, allowing for more accurate detection of true effects while maintaining the expected false positive rate under the null hypothesis.\nOverall, this simulation highlights how increased sample sizes lead to more robust and reliable conclusions in research.\n","srcMarkdownNoYaml":"\n\n# Drawing inference from statistical models, and statistical power\n\nThe study was set up as a statistical laboratory, were we performed simulations.\nThe purpose of this rapport is to interpret and explain the results we got.\n\n## Method\n\nWe simulated a population of possible values and then drew random samples, calculated statistics and interpreted them.\nThe population of values was regarded as the possible difference between two treatments in a cross-over study where participants performed both treatments.\nThe values in the population were calculated as Treatment - Control.\nWe simulated a population of one million numbers with a mean of 1.5 and a standard deviation of 3.\nWe then made two different set of studies, one set with a sample size of 8 (samp1) and one set with a sample size of 40 (samp2).\nAdditionally, we estimated the average value of the population.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\nlibrary(tidyverse)\n\nset.seed(1)\npopulation <- rnorm(1000000, mean = 1.5, sd = 3)\n\n\nsamp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n\nsamp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n\nm1 <- lm(y ~ 1, data = samp1)\nm2 <- lm(y ~ 1, data = samp2)\n\nsummary(m1)\nsummary(m2)\n```\n\nHere we drew two random samples corresponding sample sizes of 8 and 40 and saved this data in data frames with the dependent variable y.\nThen the model were fitted as a linear model and saved as a model object.\nObject m1 corresponds to a sample size of 8, while m2 corresponds to a sample size of 40.\nOur null hypothesis is that there is no difference between the two treatments.\n\n## Results\n\n### Explain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).\n\nIn our model, the estimate represents the mean of the differences between the two treatments in the cross-over study.\nIn model m1, the estimate is 1.84.\nThis means that the average difference between the two treatments for the sample of 8 participants is 1.84.\nIn model m2, the estimate is 1.5642, meaning that the average difference between the two treatments for the sample of 40 participants is a little lower than for the sample of 8.\nFurthermore, the standard error (SE) provides an estimate of how much variability we expect in the sample mean if we were to repeatedly draw samples of the same size from the population.\nIt is calculated as the sample's standard deviation (SD) divided by the square root of the sample size.\nIn m1, the standard error (SE) is 1.251, which tells us how much the sample mean of 1.84 might vary if we were to repeat the study multiple times with a sample size of 8.\nIn m2, the standard error is 0.4774 indicating that the sample size of 40 participants gives us a more precise estimate of the population mean.\nThe t-value is a ratio that compares the difference between the sample mean (estimate), and the null hypothesis relative to the standard error (SE).\nIn m1, the t.value is 1.47, meaning the observed mean difference (1.84) is 1.47 standard errors away from the null hypothesis value of 0.\nThe t.value for m2 is almost three times bigger as it is 3.276 indicating that the observed mean difference (0.4774) is 3.276 standard errors away from the null hypothesis.\nLastly, the p-value tells us the probability of observing a t-value as extreme (or more extreme) than the one calculated, assuming the null hypothesis is true (i.e., no difference between treatments).\nIn m1, the p-value is 0.185, meaning that there is an 18.5% chance of observing a difference of 1.84 or more if the true difference between the treatments was zero.\nSince this p-value is above the conventional threshold of 0.05, we fail to reject the null hypothesis, suggesting that the observed difference is not statistically significant.\nIn m2, the p-value is 0.00221, meaning that there is an 0.221% chance of observing a difference of 1.5642 or more if the true difference between the treatments was zero.\nIn comparison to m1, this p-value is below the conventional threshold of 0.05.\nWe therefore reject the null hypothesis, suggesting that the observed difference is statistically significant and there is evidence that the means differ.\n\n### Discuss what contributes to the different results in the two studies (m1 and m2).\n\nThe two studies differ primarily in sample size, where m2 have 5 times more participants than m1.\nSince m1 have a small sample, the mean might fluctuate more due to random variation, whereas larger samples (m2) tend to provide a more stable and reliable estimate closer to the true population mean [@faber2014].\nFurthermore, the larger the sample size, the smaller is the standard error, which means a more precise estimate of the population mean [@faber2014].\nIn m2, the larger sample size leads to a smaller standard error (0.4774), which reduces the uncertainty around the estimate and increases the power of the test to detect differences.\nThe t-value is influenced by both the estimate and the standard error.\nEven if the estimates are somewhat similar, the smaller standard error in m2 results in a larger t-value, making it more likely to detect a significent effect.\nAdditionally, the p-value depends on the t-value.\nWith a larger sample size, as in m2, the t-value is typically larger, leading to a smaller p-value.This means that m2 is more likely to detect significant differences than m1, where the small sample size leads to a higher p-value and lower statistical power.\nIn conclusion, the larger sample size in m2 leads to a more precise estimate, a smaller standard error, a higher t-value, and ultimately a smaller p-value, increasing the likelihood of detecting a significant difference between the treatments.\n\n### Why do we use the shaded area in the lower and upper tail of the t-distribution.\n\nThe shaded area in the lower and upper tail of the t-distribution represents the probability of observing extreme values (both high and low) of the t-value under the null hypothesis.\nThis area helps us determine the p-value, which tells us how likely it is that our observed data could occur by random chance if the null hypothesis is true.\n\nThe total shaded area in both tails represents the combined probability of observing a t-value as extreme as the one calculated, assuming the null hypothesis is true.\nThe p-value for m1 where 0.185, meaning that 18.5% of the area is in the combined tails, representing the threshold for statistical significance.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults <- bind_rows(results_8, results_40)\n\n\n```\n\n### Calculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Calculate the standard deviation of the estimate variable for sample size 8 and 40\nsd_estimate_8 <- sd(results_8$estimate)\nsd_estimate_40 <- sd(results_40$estimate)\n\n\n# Calculate the average of the SE variable for sample size 8 and 40\navg_se_8 <- mean(results_8$se)\navg_se_40 <- mean(results_40$se)\n\n\n```\n\nBy calculating the standard deviation of the estimates across the 1000 studies we get a measure of how much the sample means fluctuate between different samples of the same size.\nFor the smaller sample size of 8, the standard deviation comes out to be `r sd_estimate_8`, while for the bigger sample size of 40 it comes out to be `r sd_estimate_40`.\nFurthermore, the average standard error represents the average uncertainty of the sample mean estimate in each study.\nIt reflects the variability in the sample means and depends on the sample size.\nThe average standard error for the smaller sample size is `r avg_se_8`, and `r avg_se_40` for the bigger sample size.\nWhy are these numbers very similar?\nThe standard deviation of the estimates and the average standard error are conceptually related, as the standard error estimates how much the sample mean might vary from the population mean.\nThey are both measures of variability, but while standard error (SE) is an estimate based on the sample, the standard deviation of the estimates shows the actual observed variation across multiple studies.\nTherefore, in light of these calculations, we can define standard error (SE) as the expected variability from sample to sample.\n\n### Create a histogram of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# A two facets histogram can be created with ggplot2\nresults %>%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n\n```\n\nFor sample size 8, the p-values are more spread out, with many values above the significance threshold of 0.05.\nThis indicates that with a smaller sample, there is lower statistical power, and many studies fail to detect a statistically significant difference.\nFor sample size 40, the p-values are more concentrated towards lower values, indicating that a larger sample size increases the likelihood of detecting significant effects.\nThis reflects increased statistical power with larger sample sizes, meaning the test is more likely to reject the null hypothesis when a true effect exists.\n\n### Calculate the number of studies from each sample size that declare a statistical significant effect.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Count the proportion of tests below a certain p-value for each \nresults %>%\n  filter(pval < 0.05) %>%\n  group_by(n) %>%\n  summarise(sig_results = n()/1000)\n\n```\n\nWhen calculating the number of studies that declare a statistical significant effect we find that fewer studies (0.227) are likely to reach statistical significance in the sample size of 8.\nThis is because of the low statistical power associated with small samples.\nIn the sample size of 40, more studies (0.865) will show a significant effect due to the higher power of larger samples, making it easier to detect true differences.\n\n### Using the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Using the pwr package\nlibrary(pwr)\n\npwr.t.test(n = 8, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\npwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = \"one.sample\")\n```\n\nTo calculate the power of a one-sample t-test, we use the effect size 𝑑 = 𝜇/𝜎, where 𝜇 = 1.5 (mean) and σ = 3 (standard deviation).\nThe effect size 𝑑 = 1.5/3 = 0.5.\nFor a sample size of 8 we find that the power will be relatively low, 0.232077, reflecting that with smaller samples, we have a lower chance of detecting a true effect when it exists [@faber2014].\nThis corresponds to fewer studies achieving significance in the simulation.\nFor a sample size of 40 we find that the power will be much higher, 0.8693981, indicating that with a larger sample size, we have a higher chance of detecting a true effect.\nThis explains why more studies with sample size 40 declare significance in the simulation.\n\nWe will now simulate a population without differences between treatment and control.\nThe code below is very similar to the one we use above, except that we use an average effect of 0 in the population.\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\npopulation <- rnorm(1000000, mean = 0, sd = 3)\n\n\n# Create data frames to store the model estimates\nresults_8 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 8)  \n\nresults_40 <- data.frame(estimate = rep(NA, 1000), \n                      se = rep(NA, 1000), \n                      pval = rep(NA, 1000), \n                      n = 40)\n\n# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample\n# from the population. \n\nfor(i in 1:1000) {\n  \n  # Draw a sample \n  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))\n  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))\n\n  # Model the data\n  m1 <- lm(y ~ 1, data = samp1)\n  m2 <- lm(y ~ 1, data = samp2)\n  \n  # Extract values from the models\n  results_8[i, 1] <- coef(summary(m1))[1, 1]\n  results_8[i, 2] <- coef(summary(m1))[1, 2]\n  results_8[i, 3] <- coef(summary(m1))[1, 4]\n\n  results_40[i, 1] <- coef(summary(m2))[1, 1]\n  results_40[i, 2] <- coef(summary(m2))[1, 2]\n  results_40[i, 3] <- coef(summary(m2))[1, 4]\n  \n  \n}\n\n\n# Save the results in a combined data frame\n\nresults_null <- bind_rows(results_8, results_40)\n\n#create new histogram \n\nresults_null %>%\n  ggplot(aes(pval)) + \n  geom_histogram() +\n  facet_wrap(~ n)\n  \n```\n\n### With a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?\n\n```{r}\n#| warning: false \n#| message: false \n#| echo: false \n#| code-fold: true \n\n# Count number of false positives for sample size 8 (p-value < 0.05)\nfalse_positives_8 <- sum(results_8$pval < 0.05)\n\n# Count number of false positives for sample size 40 (p-value < 0.05)\nfalse_positives_40 <- sum(results_40$pval < 0.05)\n\n\n\n```\n\nTo determine how many studies would give a \"false positive\" result with a significance level of 5%, we calculate how many studies produce a p-value less than 0.05 in this simulation.\nSince the population mean is set to 0 any p-value below 0.05 in this scenario would represent a false positive.\nFor the sample size of 8 we got `r false_positives_8` while for the sample size of 40 we got `r false_positives_40`.\nWith a significance level of 5%, we expect roughly 5% of the 1000 studies to produce p-values below 0.05 due to random variation, even though the null hypothesis is true.\nFor 1000 studies, this means we expect around 50 false positive results for each sample size.\nThe actual number of false positives may vary slightly due to the randomness in the simulation but should be close to 50 for each sample size (8 and 40).\n\n## Conclusion\n\nIn this study, we explored the impact of sample size on statistical results by conducting simulations across different study sizes (8 and 40 participants).\nWe observed that larger sample sizes lead to more precise estimates, lower standard errors, and increased t-values, resulting in a higher likelihood of detecting statistically significant effects.\nOn the other hand, smaller sample sizes had higher variability in estimates and standard errors, contributing to lower statistical power and fewer significant results.\nOur findings also emphasize the importance of sample size in improving the reliability of statistical tests, as demonstrated by the power calculations.\nLarger samples provide greater statistical power, allowing for more accurate detection of true effects while maintaining the expected false positive rate under the null hypothesis.\nOverall, this simulation highlights how increased sample sizes lead to more robust and reliable conclusions in research.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":"html_document","warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"03-statistical-inference.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","bibliography":["resources/bib-final.bib"],"editor_options":{"chunk_output_type":"console"},"editor":{"markdown":{"wrap":"sentence","bibliography":"bibliography.bib"}}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}